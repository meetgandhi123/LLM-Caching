# LLM-Caching

The main focus for this project is to use a caching system for prompt's that are been passed to LLM models in order to reduce the number of request calls to the LLM Model and faster retrival of information. 

Tools used:
- Redis
- OpenAI ChatGPT LLM Model 
- Semantic Search

![Flowchart](https://github.com/meetgandhi123/LLM-Caching/assets/28274170/db69bf39-b23f-47c2-92ac-0e6df008a26d)
